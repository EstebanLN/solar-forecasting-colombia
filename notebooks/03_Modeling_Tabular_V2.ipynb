{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74d5237d",
   "metadata": {},
   "source": [
    "\n",
    "# 03 — Modeling Tabular"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e0b9ded",
   "metadata": {},
   "source": [
    "## Baselines + RNNs (LSTM, GRU, Dilated, Clockwork) con datos tabulados.\n",
    "\n",
    "**Optimización Bayesiana**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c1dd049",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, warnings, multiprocessing as mp, random, numpy as np, torch\n",
    "os.environ[\"PYTHONDONTWRITEBYTECODE\"] = \"1\"  \n",
    "\n",
    "try:\n",
    "    mp.set_start_method(\"spawn\", force=True)  \n",
    "except RuntimeError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4be1918",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c04a835c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os, json, math, time, random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import optuna\n",
    "from optuna.storages import JournalStorage, JournalFileStorage\n",
    "from optuna.storages.journal._file import JournalFileOpenLock\n",
    "\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6d05c9",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "801ea70e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "GPU: NVIDIA GeForce RTX 4090\n"
     ]
    }
   ],
   "source": [
    "SEED = 42\n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", DEVICE)\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "929ee7fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_CLEAN = Path(\"../data/clean/base_dataset.csv\")\n",
    "OUT_DIR = Path(\"../outputs\"); OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "ART_DIR = OUT_DIR / \"artifacts\"; ART_DIR.mkdir(parents=True, exist_ok=True)\n",
    "FIG_DIR = OUT_DIR / \"figures\"; FIG_DIR.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "55ca66d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_COL = \"GHI\"\n",
    "FREQ = \"10T\"\n",
    "INPUT_STEPS   = 36   # 6h past\n",
    "HORIZON_STEPS = 6    # 1h ahead\n",
    "BATCH_SIZE    = 256\n",
    "# EPOCHS        = 40\n",
    "PATIENCE      = 6    # Early stopping patience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ea0ad20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1886431/812373724.py:4: FutureWarning: JournalFileStorage has been deprecated in v4.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v4.0.0. Use :class:`~optuna.storages.journal.JournalFileBackend` instead.\n",
      "  STORAGE = JournalStorage(JournalFileStorage(str(JOURNAL_PATH), lock_obj=lock))\n"
     ]
    }
   ],
   "source": [
    "# Optuna runs\n",
    "JOURNAL_PATH = ART_DIR / \"optuna_tabular.journal\"\n",
    "lock = JournalFileOpenLock(str(JOURNAL_PATH))\n",
    "STORAGE = JournalStorage(JournalFileStorage(str(JOURNAL_PATH), lock_obj=lock))\n",
    "\n",
    "MODE = \"AIA\"  # \"LAPTOP\" o \"AIA\"\n",
    "\n",
    "if MODE == \"LAPTOP\":\n",
    "    N_TRIALS_RF   = 20\n",
    "    N_TRIALS_LSTM = 20\n",
    "    N_TRIALS_GRU  = 20\n",
    "    N_TRIALS_DIL  = 20\n",
    "    N_TRIALS_CW   = 20\n",
    "    EPOCHS        = 30  \n",
    "    \n",
    "elif MODE == \"AIA\":\n",
    "    N_TRIALS_RF   = 120\n",
    "    N_TRIALS_LSTM = 120\n",
    "    N_TRIALS_GRU  = 120\n",
    "    N_TRIALS_DIL  = 120\n",
    "    N_TRIALS_CW   = 120\n",
    "    EPOCHS        = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "49bd8661",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pruner (cut bad trials short)\n",
    "PRUNER = optuna.pruners.MedianPruner(n_warmup_steps=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4a014b",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d2b5b1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(DATA_CLEAN, parse_dates=[0], index_col=0).sort_index()\n",
    "df.index.name = \"time\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2ca1579f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['CSI', 'GHI', 'Presion', 'TempAmb', 'Wind Y', 'Wind X', 'DoY Sin',\n",
       "       'DoY Cos', 'horas', '__missing_target', 'flag_GHI_range',\n",
       "       'flag_TempAmb_range', 'flag_Presion_range', 'flag_CSI_range', 'hour',\n",
       "       'dow', 'month', 'minute', 'is_weekend', 'hour_sin', 'hour_cos',\n",
       "       'WindSpeed', 'WindDirection', 'GHI_roll1h_mean', 'GHI_roll3h_mean',\n",
       "       'GHI_roll6h_mean', 'GHI_roll1h_max', 'TempAmb_roll1h_mean',\n",
       "       'TempAmb_roll3h_mean', 'TempAmb_roll6h_mean', 'TempAmb_roll1h_max',\n",
       "       'Presion_roll1h_mean', 'Presion_roll3h_mean', 'Presion_roll6h_mean',\n",
       "       'Presion_roll1h_max', 'WindSpeed_roll1h_mean', 'WindSpeed_roll3h_mean',\n",
       "       'WindSpeed_roll6h_mean', 'WindSpeed_roll1h_max', 'GHI_lag1', 'GHI_lag3',\n",
       "       'GHI_lag6', 'GHI_lag12', 'GHI_lag36', 'solar_zenith', 'solar_azimuth',\n",
       "       'solar_elevation', 'ETR', 'clear_sky_ghi', 'CSI_advanced',\n",
       "       'ghi_1min_change', 'ghi_5min_std', 'ghi_persistence_1h',\n",
       "       'temp_pressure_ratio', 'wind_temp_interaction', 'wind_cloud_effect'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f8f2f2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_feats = [\n",
    "    'Presion','TempAmb','WindSpeed','WindDirection',\n",
    "    'hour_sin','hour_cos','DoY Sin','DoY Cos',\n",
    "    'solar_zenith','solar_azimuth','solar_elevation',\n",
    "    'TempAmb_roll1h_mean','TempAmb_roll6h_mean',\n",
    "    'Presion_roll1h_mean','Presion_roll6h_mean',\n",
    "    'WindSpeed_roll1h_mean','WindSpeed_roll6h_mean',\n",
    "    'temp_pressure_ratio','wind_temp_interaction'\n",
    "]\n",
    "\n",
    "ghi_lags = [c for c in ['GHI_lag1','GHI_lag3','GHI_lag6','GHI_lag12','GHI_lag36'] if c in df.columns]\n",
    "ghi_rolls = [c for c in ['GHI_roll1h_mean','GHI_roll3h_mean','GHI_roll6h_mean','GHI_roll1h_max'] if c in df.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "68406b23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total features used: 28\n",
      "['Presion', 'TempAmb', 'WindSpeed', 'WindDirection', 'hour_sin', 'hour_cos', 'DoY Sin', 'DoY Cos', 'solar_zenith', 'solar_azimuth', 'solar_elevation', 'TempAmb_roll1h_mean', 'TempAmb_roll6h_mean', 'Presion_roll1h_mean', 'Presion_roll6h_mean', 'WindSpeed_roll1h_mean', 'WindSpeed_roll6h_mean', 'temp_pressure_ratio', 'wind_temp_interaction', 'GHI_lag1', 'GHI_lag3', 'GHI_lag6', 'GHI_lag12', 'GHI_lag36', 'GHI_roll1h_mean', 'GHI_roll3h_mean', 'GHI_roll6h_mean', 'GHI_roll1h_max']\n"
     ]
    }
   ],
   "source": [
    "feat_cols = [c for c in base_feats if c in df.columns] + ghi_lags + ghi_rolls\n",
    "print(f\"Total features used: {len(feat_cols)}\")\n",
    "print(feat_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c9dc3a6",
   "metadata": {},
   "source": [
    "## Split\n",
    "\n",
    "Temporal split  70/15/15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "641bef31",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(df); i_tr = int(0.7*n); i_va = int(0.85*n)\n",
    "df_train, df_val, df_test = df.iloc[:i_tr], df.iloc[i_tr:i_va], df.iloc[i_va:]\n",
    "\n",
    "X_scaler = StandardScaler(); y_scaler = StandardScaler()\n",
    "\n",
    "X_train = X_scaler.fit_transform(df_train[feat_cols].values)\n",
    "y_train = y_scaler.fit_transform(df_train[[TARGET_COL]].values).ravel()\n",
    "X_val   = X_scaler.transform(df_val[feat_cols].values)\n",
    "y_val   = y_scaler.transform(df_val[[TARGET_COL]].values).ravel()\n",
    "X_test  = X_scaler.transform(df_test[feat_cols].values)\n",
    "y_test  = y_scaler.transform(df_test[[TARGET_COL]].values).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aafb2d58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valores NaN en X_train: 144\n",
      "Valores NaN en y_train: 0\n",
      "Valores NaN en X_val: 0\n",
      "Valores NaN en X_test: 0\n"
     ]
    }
   ],
   "source": [
    "# NaN before split\n",
    "print(\"Valores NaN en X_train:\", np.isnan(X_train).sum())\n",
    "print(\"Valores NaN en y_train:\", np.isnan(y_train).sum())\n",
    "print(\"Valores NaN en X_val:\", np.isnan(X_val).sum())\n",
    "print(\"Valores NaN en X_test:\", np.isnan(X_test).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0eebde43",
   "metadata": {},
   "outputs": [],
   "source": [
    "imp = SimpleImputer(strategy=\"median\")\n",
    "X_train = imp.fit_transform(X_train)\n",
    "X_val   = imp.transform(X_val)\n",
    "X_test  = imp.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f977df16",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, arr in [(\"X_train\",X_train),(\"X_val\",X_val),(\"X_test\",X_test),(\"y_train\",y_train),(\"y_val\",y_val),(\"y_test\",y_test)]:\n",
    "    assert np.isfinite(arr).all(), f\"{name} tiene NaN/Inf\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc9c37dc",
   "metadata": {},
   "source": [
    "## Helpers - Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eee598c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def metrics_from_scaled(pred_scaled, true_scaled, y_scaler):\n",
    "#     p = y_scaler.inverse_transform(pred_scaled.reshape(-1,1)).ravel()\n",
    "#     t = y_scaler.inverse_transform(true_scaled.reshape(-1,1)).ravel()\n",
    "#     mae = mean_absolute_error(t, p)\n",
    "#     rmse = math.sqrt(mean_squared_error(t, p))\n",
    "#     mape = np.mean(np.abs((t + 1e-6) - p) / (np.abs(t) + 1e-6)) * 100\n",
    "#     return {\"MAE\": mae, \"RMSE\": rmse, \"MAPE\": mape}, (t, p)\n",
    "\n",
    "def metrics_from_scaled(pred_scaled, true_scaled, y_scaler):\n",
    "    p = y_scaler.inverse_transform(pred_scaled.reshape(-1,1)).ravel()\n",
    "    t = y_scaler.inverse_transform(true_scaled.reshape(-1,1)).ravel()\n",
    "    mae = mean_absolute_error(t, p)\n",
    "    rmse = math.sqrt(mean_squared_error(t, p))\n",
    "    mape = np.mean(np.abs((t + 1e-6) - p) / (np.abs(t) + 1e-6)) * 100\n",
    "    smape = 100 * np.mean(2*np.abs(p - t) / (np.abs(t) + np.abs(p) + 1e-6))\n",
    "    r2 = r2_score(t, p)\n",
    "    return {\"MAE\": mae, \"RMSE\": rmse, \"MAPE\": mape, \"sMAPE\": smape, \"R2\": r2}, (t, p)\n",
    "\n",
    "def persistence_baseline(y_scaled, horizon):\n",
    "    # ŷ(t+h) = y(t) en espacio ESCALADO \n",
    "    y_hat = np.roll(y_scaled, horizon)\n",
    "    y_hat[:horizon] = y_scaled[horizon]  # simple fill\n",
    "    return y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3ca3e7",
   "metadata": {},
   "source": [
    "## Baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "956f0efc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Persistence: {'MAE': 87.45895617690968, 'RMSE': 146.0029059609885, 'MAPE': np.float64(60136261.87772505), 'sMAPE': np.float64(71.63856028655798), 'R2': 0.7978191012356063}\n",
      "Linear     : {'MAE': 25.688872618558197, 'RMSE': 53.7405079013805, 'MAPE': np.float64(194256322.02786574), 'sMAPE': np.float64(104.16269303777979), 'R2': 0.9726081946776229}\n",
      "RF baseline: {'MAE': 20.102909392708263, 'RMSE': 52.42468476035881, 'MAPE': np.float64(5408558.475804609), 'sMAPE': np.float64(52.443049275776396), 'R2': 0.9739331364582189}\n"
     ]
    }
   ],
   "source": [
    "lin = LinearRegression().fit(X_train, y_train)\n",
    "lin_metrics, (y_true_lin, y_pred_lin) = metrics_from_scaled(lin.predict(X_test), y_test, y_scaler)\n",
    "\n",
    "rf0 = RandomForestRegressor(n_estimators=300, random_state=SEED, n_jobs=-1).fit(X_train, y_train)\n",
    "rf0_metrics, (y_true_rf0, y_pred_rf0) = metrics_from_scaled(rf0.predict(X_test), y_test, y_scaler)\n",
    "\n",
    "y_pers_test = persistence_baseline(y_test, HORIZON_STEPS)\n",
    "pers_metrics, (y_true_pers, y_pred_pers) = metrics_from_scaled(y_pers_test, y_test, y_scaler)\n",
    "\n",
    "print(\"Persistence:\", pers_metrics)\n",
    "print(\"Linear     :\", lin_metrics)\n",
    "print(\"RF baseline:\", rf0_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a95e8457",
   "metadata": {},
   "source": [
    "## Sequentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2112369b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_loaders_from_arrays(X_tr, y_tr, X_va, y_va, steps, horizon, batch=256):\n",
    "    ds_tr = SeqDataset(X_tr, y_tr, steps, horizon)\n",
    "    ds_va = SeqDataset(X_va, y_va, steps, horizon)\n",
    "    return (DataLoader(ds_tr, batch_size=batch, shuffle=True, drop_last=True),\n",
    "            DataLoader(ds_va, batch_size=batch, shuffle=False, drop_last=False))\n",
    "\n",
    "def make_loaders(X_tr, y_tr, X_va, y_va, X_te, y_te, steps, horizon, batch=256):\n",
    "    ds_tr = SeqDataset(X_tr, y_tr, steps, horizon)\n",
    "    ds_va = SeqDataset(X_va, y_va, steps, horizon)\n",
    "    ds_te = SeqDataset(X_te, y_te, steps, horizon)\n",
    "    return (DataLoader(ds_tr, batch_size=batch, shuffle=True, drop_last=True),\n",
    "            DataLoader(ds_va, batch_size=batch, shuffle=False, drop_last=False),\n",
    "            DataLoader(ds_te, batch_size=batch, shuffle=False, drop_last=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "65de131d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SeqDataset(Dataset):\n",
    "    def __init__(self, X, y, input_steps=36, horizon=6):\n",
    "        self.X, self.y = X, y\n",
    "        self.input_steps, self.horizon = input_steps, horizon\n",
    "        self.max_i = len(X) - input_steps - horizon\n",
    "        assert self.max_i > 0, \"No hay suficientes muestras para ventanas.\"\n",
    "    def __len__(self): return self.max_i\n",
    "    def __getitem__(self, idx):\n",
    "        i0, i1 = idx, idx + self.input_steps\n",
    "        ih = i1 + self.horizon - 1\n",
    "        return (torch.tensor(self.X[i0:i1], dtype=torch.float32),\n",
    "                torch.tensor(self.y[ih], dtype=torch.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a6e703e",
   "metadata": {},
   "source": [
    "### PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9e4116f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, in_dim, hidden=64, num_layers=1, dropout=0.0, bidirectional=False):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.LSTM(in_dim, hidden, num_layers=num_layers, batch_first=True,\n",
    "                           dropout=(dropout if num_layers>1 else 0.0), bidirectional=bidirectional)\n",
    "        out_dim = hidden * (2 if bidirectional else 1)\n",
    "        self.fc = nn.Linear(out_dim, 1)\n",
    "    def forward(self, x):\n",
    "        out, _ = self.rnn(x)\n",
    "        return self.fc(out[:, -1, :]).squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "91a8be29",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUModel(nn.Module):\n",
    "    def __init__(self, in_dim, hidden=64, num_layers=1, dropout=0.0, bidirectional=False):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.GRU(in_dim, hidden, num_layers=num_layers, batch_first=True,\n",
    "                          dropout=(dropout if num_layers>1 else 0.0), bidirectional=bidirectional)\n",
    "        out_dim = hidden * (2 if bidirectional else 1)\n",
    "        self.fc = nn.Linear(out_dim, 1)\n",
    "    def forward(self, x):\n",
    "        out, _ = self.rnn(x)\n",
    "        return self.fc(out[:, -1, :]).squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2afae3c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DilatedRNNCell(nn.Module):\n",
    "    def __init__(self, in_dim, hidden):\n",
    "        super().__init__()\n",
    "        self.Wx = nn.Linear(in_dim, hidden)\n",
    "        self.Wh = nn.Linear(hidden, hidden)\n",
    "        self.act = nn.Tanh()\n",
    "    def forward(self, x_t, h):\n",
    "        return self.act(self.Wx(x_t) + self.Wh(h))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6ac5a568",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DilatedRNN(nn.Module):\n",
    "    def __init__(self, in_dim, hidden=64, dilation=2, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.cell = DilatedRNNCell(in_dim, hidden)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(hidden, 1)\n",
    "        self.d = max(1, int(dilation))\n",
    "        self.hidden_size = hidden\n",
    "    def forward(self, x):\n",
    "        B, T, F = x.size()\n",
    "        h = torch.zeros(B, self.hidden_size, device=x.device)\n",
    "        for t in range(0, T, self.d):\n",
    "            h = self.cell(x[:, t, :], h)\n",
    "            h = self.dropout(h)\n",
    "        return self.fc(h).squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "495d593e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClockworkRNN(nn.Module):\n",
    "    def __init__(self, in_dim, hidden=60, modules=3, base_period=1, dropout=0.0):\n",
    "        super().__init__()\n",
    "        assert hidden % modules == 0, \"hidden debe ser múltiplo de modules\"\n",
    "        self.modules = modules\n",
    "        self.h_per = hidden // modules\n",
    "        self.periods = [base_period * (2**m) for m in range(modules)]\n",
    "        self.Wx = nn.ModuleList([nn.Linear(in_dim, self.h_per) for _ in range(modules)])\n",
    "        self.Wh = nn.ModuleList([nn.Linear(self.h_per, self.h_per) for _ in range(modules)])\n",
    "        self.act = nn.Tanh()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc  = nn.Linear(hidden, 1)\n",
    "    def forward(self, x):\n",
    "        B, T, F = x.size()\n",
    "        hs = [torch.zeros(B, self.h_per, device=x.device) for _ in range(self.modules)]\n",
    "        for t in range(T):\n",
    "            for m in range(self.modules):\n",
    "                if t % self.periods[m] == 0:\n",
    "                    hs[m] = self.act(self.Wx[m](x[:, t, :]) + self.Wh[m](hs[m]))\n",
    "                    hs[m] = self.dropout(hs[m])\n",
    "        h_all = torch.cat(hs, dim=1)\n",
    "        return self.fc(h_all).squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "084eff60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class LSTMModel(nn.Module):\n",
    "#     def __init__(self, in_dim, hidden=64, num_layers=1, dropout=0.0, bidirectional=False):\n",
    "#         super().__init__()\n",
    "#         self.rnn = nn.LSTM(in_dim, hidden, num_layers=num_layers, batch_first=True,\n",
    "#                            dropout=(dropout if num_layers>1 else 0.0), bidirectional=bidirectional)\n",
    "#         out_dim = hidden * (2 if bidirectional else 1)\n",
    "#         self.fc = nn.Linear(out_dim, 1)\n",
    "#     def forward(self, x):\n",
    "#         out, _ = self.rnn(x)\n",
    "#         return self.fc(out[:, -1, :]).squeeze(1)\n",
    "\n",
    "# class GRUModel(nn.Module):\n",
    "#     def __init__(self, in_dim, hidden=64, num_layers=1, dropout=0.0, bidirectional=False):\n",
    "#         super().__init__()\n",
    "#         self.rnn = nn.GRU(in_dim, hidden, num_layers=num_layers, batch_first=True,\n",
    "#                           dropout=(dropout if num_layers>1 else 0.0), bidirectional=bidirectional)\n",
    "#         out_dim = hidden * (2 if bidirectional else 1)\n",
    "#         self.fc = nn.Linear(out_dim, 1)\n",
    "#     def forward(self, x):\n",
    "#         out, _ = self.rnn(x)\n",
    "#         return self.fc(out[:, -1, :]).squeeze(1)\n",
    "\n",
    "# # Dilated RNN sencilla: procesa cada d-ésimo paso\n",
    "# class DilatedRNNCell(nn.Module):\n",
    "#     def __init__(self, in_dim, hidden):\n",
    "#         super().__init__()\n",
    "#         self.Wx = nn.Linear(in_dim, hidden)\n",
    "#         self.Wh = nn.Linear(hidden, hidden)\n",
    "#         self.act = nn.Tanh()\n",
    "#     def forward(self, x_t, h):\n",
    "#         return self.act(self.Wx(x_t) + self.Wh(h))\n",
    "\n",
    "# class DilatedRNN(nn.Module):\n",
    "#     def __init__(self, in_dim, hidden=64, dilation=2, dropout=0.0):\n",
    "#         super().__init__()\n",
    "#         self.cell = DilatedRNNCell(in_dim, hidden)\n",
    "#         self.dropout = nn.Dropout(dropout)\n",
    "#         self.fc = nn.Linear(hidden, 1)\n",
    "#         self.d = max(1, int(dilation))\n",
    "#         self.hidden_size = hidden\n",
    "#     def forward(self, x):\n",
    "#         B, T, F = x.size()\n",
    "#         h = torch.zeros(B, self.hidden_size, device=x.device)\n",
    "#         for t in range(0, T, self.d):\n",
    "#             h = self.cell(x[:, t, :], h)\n",
    "#             h = self.dropout(h)\n",
    "#         return self.fc(h).squeeze(1)\n",
    "# class ClockworkRNN(nn.Module):\n",
    "#     def __init__(self, in_dim, hidden=60, modules=3, base_period=1, dropout=0.0):\n",
    "#         super().__init__()\n",
    "#         assert hidden % modules == 0, \"hidden debe ser múltiplo de modules\"\n",
    "#         self.modules = modules\n",
    "#         self.h_per = hidden // modules\n",
    "#         # periodos tipo 1, 2, 4... * base_period\n",
    "#         self.periods = [base_period * (2**m) for m in range(modules)]\n",
    "#         self.Wx = nn.ModuleList([nn.Linear(in_dim, self.h_per) for _ in range(modules)])\n",
    "#         self.Wh = nn.ModuleList([nn.Linear(self.h_per, self.h_per) for _ in range(modules)])\n",
    "#         self.act = nn.Tanh()\n",
    "#         self.dropout = nn.Dropout(dropout)\n",
    "#         self.fc  = nn.Linear(hidden, 1)\n",
    "#     def forward(self, x):\n",
    "#         B, T, F = x.size()\n",
    "#         hs = [torch.zeros(B, self.h_per, device=x.device) for _ in range(self.modules)]\n",
    "#         for t in range(T):\n",
    "#             for m in range(self.modules):\n",
    "#                 if t % self.periods[m] == 0:\n",
    "#                     hs[m] = self.act(self.Wx[m](x[:, t, :]) + self.Wh[m](hs[m]))\n",
    "#                     hs[m] = self.dropout(hs[m])\n",
    "#         h_all = torch.cat(hs, dim=1)\n",
    "#         return self.fc(h_all).squeeze(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8506b8b",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0251fb85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_torch_model(model, dl_train, dl_val, epochs=40, lr=1e-3, patience=6, device=DEVICE, trial=None, weight_decay=0.0):\n",
    "    model = model.to(device)\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    sched = torch.optim.lr_scheduler.ReduceLROnPlateau(opt, mode=\"min\", factor=0.5, patience=2, verbose=False)\n",
    "    loss_fn = nn.MSELoss()\n",
    "    best_val, best_state, no_improve = float(\"inf\"), None, 0\n",
    "\n",
    "    for ep in range(1, epochs+1):\n",
    "        model.train(); tr_losses=[]\n",
    "        for xb, yb in dl_train:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            opt.zero_grad(); pred = model(xb)\n",
    "            loss = loss_fn(pred, yb); loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            opt.step()\n",
    "            tr_losses.append(loss.item())\n",
    "\n",
    "        model.eval(); va_losses=[]\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in dl_val:\n",
    "                xb, yb = xb.to(device), yb.to(device)\n",
    "                va_losses.append(loss_fn(model(xb), yb).item())\n",
    "        val_mse = float(np.mean(va_losses))\n",
    "        sched.step(val_mse)\n",
    "\n",
    "        if trial is not None:\n",
    "            trial.report(val_mse, ep)\n",
    "            if trial.should_prune(): raise optuna.TrialPruned()\n",
    "\n",
    "        if val_mse < best_val - 1e-6:\n",
    "            best_val, best_state, no_improve = val_mse, model.state_dict(), 0\n",
    "        else:\n",
    "            no_improve += 1\n",
    "            if no_improve >= patience: break\n",
    "\n",
    "    if best_state is not None: model.load_state_dict(best_state)\n",
    "    return model, best_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0e5eb616",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def eval_sequence_model(model, dl, y_scaler, device=DEVICE):\n",
    "#     model.eval(); preds, trues = [], []\n",
    "#     with torch.no_grad():\n",
    "#         for xb, yb in dl:\n",
    "#             xb = xb.to(device)\n",
    "#             preds.append(model(xb).cpu().numpy())\n",
    "#             trues.append(yb.cpu().numpy())\n",
    "#     p = np.concatenate(preds); t = np.concatenate(trues)\n",
    "#     p_o = y_scaler.inverse_transform(p.reshape(-1,1)).ravel()\n",
    "#     t_o = y_scaler.inverse_transform(t.reshape(-1,1)).ravel()\n",
    "#     mae  = mean_absolute_error(t_o, p_o)\n",
    "#     rmse = math.sqrt(mean_squared_error(t_o, p_o))\n",
    "#     mape = np.mean(np.abs((t_o + 1e-6) - p_o) / (np.abs(t_o) + 1e-6)) * 100\n",
    "#     return {\"MAE\":mae, \"RMSE\":rmse, \"MAPE\":mape}, (t_o, p_o)\n",
    "def eval_sequence_model(model, dl, y_scaler, device=DEVICE):\n",
    "    model.eval(); preds, trues = [], []\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in dl:\n",
    "            xb = xb.to(device)\n",
    "            preds.append(model(xb).cpu().numpy())\n",
    "            trues.append(yb.cpu().numpy())\n",
    "    p = np.concatenate(preds); t = np.concatenate(trues)\n",
    "    p_o = y_scaler.inverse_transform(p.reshape(-1,1)).ravel()\n",
    "    t_o = y_scaler.inverse_transform(t.reshape(-1,1)).ravel()\n",
    "    mae  = mean_absolute_error(t_o, p_o)\n",
    "    rmse = math.sqrt(mean_squared_error(t_o, p_o))\n",
    "    mape = np.mean(np.abs((t_o + 1e-6) - p_o) / (np.abs(t_o) + 1e-6)) * 100\n",
    "    smape = 100 * np.mean(2*np.abs(p_o - t_o) / (np.abs(t_o) + np.abs(p_o) + 1e-6))\n",
    "    r2 = r2_score(t_o, p_o)\n",
    "    return {\"MAE\":mae, \"RMSE\":rmse, \"MAPE\":mape, \"sMAPE\": smape, \"R2\": r2}, (t_o, p_o)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04811ade",
   "metadata": {},
   "source": [
    "## Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d5ff2898",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _create_study(name: str):\n",
    "    return optuna.create_study(\n",
    "        direction=\"minimize\",\n",
    "        study_name=name,\n",
    "        pruner=PRUNER,\n",
    "        storage=STORAGE,\n",
    "        load_if_exists=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb2b541",
   "metadata": {},
   "source": [
    "#### RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "82f38df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_rf(trial):\n",
    "    rf = RandomForestRegressor(\n",
    "        n_estimators      = trial.suggest_int(\"n_estimators\", 200, 700),\n",
    "        max_depth         = trial.suggest_int(\"max_depth\", 6, 28),\n",
    "        min_samples_split = trial.suggest_int(\"min_samples_split\", 2, 20),\n",
    "        min_samples_leaf  = trial.suggest_int(\"min_samples_leaf\", 1, 10),\n",
    "        n_jobs=-1, random_state=SEED\n",
    "    )\n",
    "    rf.fit(X_train, y_train)\n",
    "    pred_val = rf.predict(X_val)\n",
    "    p_o = y_scaler.inverse_transform(pred_val.reshape(-1,1)).ravel()\n",
    "    t_o = y_scaler.inverse_transform(y_val.reshape(-1,1)).ravel()\n",
    "    return math.sqrt(mean_squared_error(t_o, p_o))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eabc4cec",
   "metadata": {},
   "source": [
    "#### RNN\n",
    "(causals: bidirectional=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ff53f869",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_rnn_builder(model_kind=\"LSTM\"):\n",
    "    def _obj(trial):\n",
    "        hidden  = trial.suggest_int(\"hidden\", 64, 512, step=32)\n",
    "        layers  = trial.suggest_int(\"num_layers\", 1, 3)\n",
    "        dropout = trial.suggest_float(\"dropout\", 0.0, 0.6)\n",
    "        bidir   = False  # causal para operación real\n",
    "        lr      = trial.suggest_float(\"lr\", 1e-4, 5e-3, log=True)\n",
    "        steps   = trial.suggest_categorical(\"input_steps\", [24, 36, 48, 60, 72])\n",
    "        horizon = trial.suggest_categorical(\"horizon_steps\", [3, 6, 12])\n",
    "        batch   = trial.suggest_categorical(\"batch\", [128, 256, 512])\n",
    "        wd      = trial.suggest_float(\"weight_decay\", 0.0, 1e-3) \n",
    "\n",
    "        dl_tr, dl_va = make_loaders_from_arrays(X_train, y_train, X_val, y_val, steps, horizon, batch=batch)\n",
    "        in_dim = X_train.shape[1]\n",
    "        model = LSTMModel(in_dim, hidden, layers, dropout, bidir) if model_kind==\"LSTM\" else GRUModel(in_dim, hidden, layers, dropout, bidir)\n",
    "\n",
    "        _, best_val = train_torch_model(model, dl_tr, dl_va, epochs=EPOCHS, lr=lr, patience=PATIENCE, device=DEVICE, trial=trial, weight_decay=wd)\n",
    "        return best_val\n",
    "    return _obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "15a4ab0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_dilated(trial: optuna.Trial):\n",
    "    hidden  = trial.suggest_int(\"hidden\", 64, 512, step=32)\n",
    "    dilation= trial.suggest_categorical(\"dilation\", [1, 2, 3, 4, 6])\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.0, 0.6)\n",
    "    lr      = trial.suggest_float(\"lr\", 1e-4, 5e-3, log=True)\n",
    "    steps   = trial.suggest_categorical(\"input_steps\", [24, 36, 48, 60, 72])\n",
    "    horizon = trial.suggest_categorical(\"horizon_steps\", [3, 6, 12])\n",
    "    batch   = trial.suggest_categorical(\"batch\", [128, 256, 512])\n",
    "    wd      = trial.suggest_float(\"weight_decay\", 0.0, 1e-3)\n",
    "\n",
    "    dl_tr, dl_va = make_loaders_from_arrays(X_train, y_train, X_val, y_val, steps, horizon, batch=batch)\n",
    "    model = DilatedRNN(X_train.shape[1], hidden=hidden, dilation=dilation, dropout=dropout)\n",
    "    _, best_val = train_torch_model(model, dl_tr, dl_va, epochs=EPOCHS, lr=lr, patience=PATIENCE, device=DEVICE, trial=trial, weight_decay=wd)\n",
    "    return best_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8f8011e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_clockwork(trial: optuna.Trial):\n",
    "    hidden  = trial.suggest_int(\"hidden\", 90, 300, step=30)\n",
    "    modules = trial.suggest_categorical(\"modules\", [3, 4, 5])\n",
    "    if hidden % modules != 0: raise optuna.TrialPruned()\n",
    "    base_period = trial.suggest_categorical(\"base_period\", [1, 2])\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.0, 0.6)\n",
    "    lr      = trial.suggest_float(\"lr\", 1e-4, 5e-3, log=True)\n",
    "    steps   = trial.suggest_categorical(\"input_steps\", [24, 36, 48, 60, 72])\n",
    "    horizon = trial.suggest_categorical(\"horizon_steps\", [3, 6, 12])\n",
    "    batch   = trial.suggest_categorical(\"batch\", [128, 256, 512])\n",
    "    wd      = trial.suggest_float(\"weight_decay\", 0.0, 1e-3)\n",
    "\n",
    "    dl_tr, dl_va = make_loaders_from_arrays(X_train, y_train, X_val, y_val, steps, horizon, batch=batch)\n",
    "    model = ClockworkRNN(X_train.shape[1], hidden=hidden, modules=modules, base_period=base_period, dropout=dropout)\n",
    "    _, best_val = train_torch_model(model, dl_tr, dl_va, epochs=EPOCHS, lr=lr, patience=PATIENCE, device=DEVICE, trial=trial, weight_decay=wd)\n",
    "    return best_val"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df937830",
   "metadata": {},
   "source": [
    "### Run Studies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7b59152a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# study_rf   = _create_study(\"RF_RMSE\");      study_rf.optimize(objective_rf, n_trials=N_TRIALS_RF)\n",
    "# study_lstm = _create_study(\"LSTM_MSEval\");  study_lstm.optimize(objective_rnn_builder(\"LSTM\"), n_trials=N_TRIALS_LSTM)\n",
    "# study_gru  = _create_study(\"GRU_MSEval\");   study_gru.optimize(objective_rnn_builder(\"GRU\"),  n_trials=N_TRIALS_GRU)\n",
    "# study_dil  = _create_study(\"DilatedRNN_MSEval\"); study_dil.optimize(objective_dilated,     n_trials=N_TRIALS_DIL)\n",
    "# study_cw   = _create_study(\"ClockworkRNN_MSEval\"); study_cw.optimize(objective_clockwork,  n_trials=N_TRIALS_CW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324f88d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a64fdc481414dd18c58b01637840ff3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/120 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "study_rf   = _create_study(\"RF_RMSE\")\n",
    "study_rf.optimize(objective_rf, n_trials=N_TRIALS_RF, gc_after_trial=True, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6a778a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03b4eb1ba3b54391aa3e7221ed796331",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/120 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W 2025-10-10 16:40:04,227] Trial 0 failed with parameters: {'hidden': 480, 'num_layers': 1, 'dropout': 0.05162991191370032, 'lr': 0.00031095669409816944, 'input_steps': 24, 'horizon_steps': 3, 'batch': 128, 'weight_decay': 2.448164099733641e-05} because of the following error: TypeError(\"ReduceLROnPlateau.__init__() got an unexpected keyword argument 'verbose'\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/e.ladino/miniconda3/envs/e_ladino/lib/python3.12/site-packages/optuna/study/_optimize.py\", line 201, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_1878720/2815648342.py\", line 17, in _obj\n",
      "    _, best_val = train_torch_model(model, dl_tr, dl_va, epochs=EPOCHS, lr=lr, patience=PATIENCE, device=DEVICE, trial=trial, weight_decay=wd)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_1878720/2362540632.py\", line 4, in train_torch_model\n",
      "    sched = torch.optim.lr_scheduler.ReduceLROnPlateau(opt, mode=\"min\", factor=0.5, patience=2, verbose=False)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "TypeError: ReduceLROnPlateau.__init__() got an unexpected keyword argument 'verbose'\n",
      "[W 2025-10-10 16:40:04,228] Trial 0 failed with value None.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "ReduceLROnPlateau.__init__() got an unexpected keyword argument 'verbose'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[35]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m study_lstm = _create_study(\u001b[33m\"\u001b[39m\u001b[33mLSTM_MSEval\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mstudy_lstm\u001b[49m\u001b[43m.\u001b[49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective_rnn_builder\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mLSTM\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m=\u001b[49m\u001b[43mN_TRIALS_LSTM\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/e_ladino/lib/python3.12/site-packages/optuna/study/study.py:490\u001b[39m, in \u001b[36mStudy.optimize\u001b[39m\u001b[34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[39m\n\u001b[32m    388\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34moptimize\u001b[39m(\n\u001b[32m    389\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    390\u001b[39m     func: ObjectiveFuncType,\n\u001b[32m   (...)\u001b[39m\u001b[32m    397\u001b[39m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    398\u001b[39m ) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    399\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[32m    400\u001b[39m \n\u001b[32m    401\u001b[39m \u001b[33;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    488\u001b[39m \u001b[33;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[32m    489\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m490\u001b[39m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    491\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    492\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    493\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    494\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    495\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    496\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    497\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    498\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    499\u001b[39m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    500\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/e_ladino/lib/python3.12/site-packages/optuna/study/_optimize.py:63\u001b[39m, in \u001b[36m_optimize\u001b[39m\u001b[34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[39m\n\u001b[32m     61\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     62\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs == \u001b[32m1\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m63\u001b[39m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     64\u001b[39m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     65\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     66\u001b[39m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     67\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     68\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     69\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     70\u001b[39m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     71\u001b[39m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     72\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     73\u001b[39m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     74\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     75\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     76\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs == -\u001b[32m1\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/e_ladino/lib/python3.12/site-packages/optuna/study/_optimize.py:160\u001b[39m, in \u001b[36m_optimize_sequential\u001b[39m\u001b[34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[39m\n\u001b[32m    157\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m    159\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m160\u001b[39m     frozen_trial_id = \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    161\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    162\u001b[39m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[32m    163\u001b[39m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[32m    164\u001b[39m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[32m    165\u001b[39m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[32m    166\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/e_ladino/lib/python3.12/site-packages/optuna/study/_optimize.py:258\u001b[39m, in \u001b[36m_run_trial\u001b[39m\u001b[34m(study, func, catch)\u001b[39m\n\u001b[32m    251\u001b[39m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mShould not reach.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    253\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    254\u001b[39m     updated_state == TrialState.FAIL\n\u001b[32m    255\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    256\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[32m    257\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m258\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[32m    259\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m trial._trial_id\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/e_ladino/lib/python3.12/site-packages/optuna/study/_optimize.py:201\u001b[39m, in \u001b[36m_run_trial\u001b[39m\u001b[34m(study, func, catch)\u001b[39m\n\u001b[32m    199\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial._trial_id, study._storage):\n\u001b[32m    200\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m201\u001b[39m         value_or_values = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    202\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions.TrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    203\u001b[39m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[32m    204\u001b[39m         state = TrialState.PRUNED\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 17\u001b[39m, in \u001b[36mobjective_rnn_builder.<locals>._obj\u001b[39m\u001b[34m(trial)\u001b[39m\n\u001b[32m     14\u001b[39m in_dim = X_train.shape[\u001b[32m1\u001b[39m]\n\u001b[32m     15\u001b[39m model = LSTMModel(in_dim, hidden, layers, dropout, bidir) \u001b[38;5;28;01mif\u001b[39;00m model_kind==\u001b[33m\"\u001b[39m\u001b[33mLSTM\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m GRUModel(in_dim, hidden, layers, dropout, bidir)\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m _, best_val = \u001b[43mtrain_torch_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdl_tr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdl_va\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatience\u001b[49m\u001b[43m=\u001b[49m\u001b[43mPATIENCE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwd\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m best_val\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 4\u001b[39m, in \u001b[36mtrain_torch_model\u001b[39m\u001b[34m(model, dl_train, dl_val, epochs, lr, patience, device, trial, weight_decay)\u001b[39m\n\u001b[32m      2\u001b[39m model = model.to(device)\n\u001b[32m      3\u001b[39m opt = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m sched = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43moptim\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlr_scheduler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mReduceLROnPlateau\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmin\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfactor\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatience\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m loss_fn = nn.MSELoss()\n\u001b[32m      6\u001b[39m best_val, best_state, no_improve = \u001b[38;5;28mfloat\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33minf\u001b[39m\u001b[33m\"\u001b[39m), \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[32m0\u001b[39m\n",
      "\u001b[31mTypeError\u001b[39m: ReduceLROnPlateau.__init__() got an unexpected keyword argument 'verbose'"
     ]
    }
   ],
   "source": [
    "study_lstm = _create_study(\"LSTM_MSEval\")\n",
    "study_lstm.optimize(objective_rnn_builder(\"LSTM\"), n_trials=N_TRIALS_LSTM, gc_after_trial=True, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99bea0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "study_gru  = _create_study(\"GRU_MSEval\")\n",
    "study_gru.optimize(objective_rnn_builder(\"GRU\"),  n_trials=N_TRIALS_GRU, gc_after_trial=True, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39dfd9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "study_dil  = _create_study(\"DilatedRNN_MSEval\")\n",
    "study_dil.optimize(objective_dilated, n_trials=N_TRIALS_DIL, gc_after_trial=True, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0d1578",
   "metadata": {},
   "outputs": [],
   "source": [
    "study_cw   = _create_study(\"ClockworkRNN_MSEval\")\n",
    "study_cw.optimize(objective_clockwork, n_trials=N_TRIALS_CW, gc_after_trial=True, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0747fc",
   "metadata": {},
   "source": [
    "## Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14cab37",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best LSTM      :\", study_lstm.best_trial.params)\n",
    "print(\"Best GRU       :\", study_gru.best_trial.params)\n",
    "print(\"Best Dilated   :\", study_dil.best_trial.params)\n",
    "print(\"Best Clockwork :\", study_cw.best_trial.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837d9ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_rf = RandomForestRegressor(random_state=SEED, n_jobs=-1, **study_rf.best_trial.params)\n",
    "best_rf.fit(np.vstack([X_train, X_val]), np.concatenate([y_train, y_val]))\n",
    "rf_opt_metrics, (y_true_rf_opt, y_pred_rf_opt) = metrics_from_scaled(best_rf.predict(X_test), y_test, y_scaler)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed1b960",
   "metadata": {},
   "source": [
    "## Retraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "850bd218",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrain_and_test_build(build_fn, best_params, label):\n",
    "    steps, horizon = best_params[\"input_steps\"], best_params[\"horizon_steps\"]\n",
    "    batch = best_params[\"batch\"]; lr = best_params[\"lr\"]\n",
    "    wd    = best_params.get(\"weight_decay\", 0.0)\n",
    "    dl_tr, dl_va, dl_te = make_loaders(\n",
    "        np.vstack([X_train, X_val]), np.concatenate([y_train, y_val]),\n",
    "        X_val, y_val, X_test, y_test, steps, horizon, batch\n",
    "    )\n",
    "    model = build_fn(best_params)\n",
    "    model, best_val = train_torch_model(model, dl_tr, dl_va, epochs=EPOCHS, lr=lr, patience=PATIENCE, device=DEVICE, weight_decay=wd)\n",
    "    torch.save(model.state_dict(), ART_DIR / f\"best_{label}.pt\")\n",
    "    return eval_sequence_model(model, dl_te, y_scaler)\n",
    "\n",
    "in_dim = X_train.shape[1]\n",
    "build_lstm = lambda p: LSTMModel(in_dim, p[\"hidden\"], p[\"num_layers\"], p[\"dropout\"], False)\n",
    "build_gru  = lambda p: GRUModel(in_dim,  p[\"hidden\"], p[\"num_layers\"], p[\"dropout\"], False)\n",
    "build_dil  = lambda p: DilatedRNN(in_dim, p[\"hidden\"], p[\"dilation\"], p[\"dropout\"])\n",
    "build_cw   = lambda p: ClockworkRNN(in_dim, p[\"hidden\"], p[\"modules\"], p[\"base_period\"], p[\"dropout\"])\n",
    "\n",
    "lstm_metrics, (yt_lstm, yp_lstm) = retrain_and_test_build(build_lstm, study_lstm.best_trial.params, \"lstm\")\n",
    "gru_metrics,  (yt_gru,  yp_gru)  = retrain_and_test_build(build_gru,  study_gru.best_trial.params,  \"gru\")\n",
    "dil_metrics,  (yt_dil,  yp_dil)  = retrain_and_test_build(build_dil,  study_dil.best_trial.params,  \"dilated\")\n",
    "cw_metrics,   (yt_cw,   yp_cw)   = retrain_and_test_build(build_cw,   study_cw.best_trial.params,   \"clockwork\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6a8805",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5c4dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {\n",
    "    \"Persistence\": pers_metrics,\n",
    "    \"LinearRegression\": lin_metrics,\n",
    "    \"RandomForest_baseline\": rf0_metrics,\n",
    "    \"RandomForest_Optuna\": rf_opt_metrics,\n",
    "    \"LSTM_Optuna\": lstm_metrics,\n",
    "    \"GRU_Optuna\":  gru_metrics,\n",
    "    \"DilatedRNN_Optuna\": dil_metrics,\n",
    "    \"ClockworkRNN_Optuna\": cw_metrics,\n",
    "}\n",
    "res_df = pd.DataFrame(results).T.sort_values(\"RMSE\")\n",
    "display(res_df.round(3))\n",
    "\n",
    "with open(ART_DIR/\"tabular_results_optuna.json\",\"w\") as f:\n",
    "    json.dump({k:{m:float(vv) for m,vv in v.items()} for k,v in results.items()}, f, indent=2)\n",
    "print(\"Saved:\", ART_DIR/\"tabular_results_optuna.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf640a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(ART_DIR/\"tabular_results_optuna.json\",\"w\") as f:\n",
    "    json.dump({k:{m:float(vv) for m,vv in v.items()} for k,v in results.items()}, f, indent=2)\n",
    "print(\"Saved:\", ART_DIR/\"tabular_results_optuna.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b03326",
   "metadata": {},
   "source": [
    "## Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc4bd29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_sample(y_true, y_pred, title, n=1000):\n",
    "    n = min(n, len(y_true))\n",
    "    plt.figure(figsize=(11,3.8))\n",
    "    plt.plot(y_true[:n], label=\"Real\", lw=1.5)\n",
    "    plt.plot(y_pred[:n], label=\"Pred\", lw=1.2, alpha=0.9)\n",
    "    plt.title(title); plt.xlabel(\"Time steps (10-min)\"); plt.ylabel(\"GHI (W/m²)\")\n",
    "    plt.legend(frameon=False); plt.tight_layout()\n",
    "\n",
    "plot_sample(y_true_rf_opt, y_pred_rf_opt, \"RandomForest Optuna — Test (sample)\")\n",
    "plt.savefig(FIG_DIR / \"pred_rf_opt_sample.png\"); plt.show()\n",
    "\n",
    "for name, (yt, yp) in {\n",
    "    \"LSTM\": (yt_lstm, yp_lstm),\n",
    "    \"GRU\":  (yt_gru,  yp_gru),\n",
    "    \"Dilated\": (yt_dil, yp_dil),\n",
    "    \"Clockwork\": (yt_cw, yp_cw),\n",
    "}.items():\n",
    "    plot_sample(yt, yp, f\"{name} — Test (sample)\")\n",
    "    plt.savefig(FIG_DIR / f\"pred_{name.lower()}_sample.png\"); plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "e_ladino",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
