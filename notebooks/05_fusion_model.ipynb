{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6adda39f",
   "metadata": {},
   "source": [
    "# 05 — Modeling Satellite + Tabular\n",
    "\n",
    "## Baselines + Models\n",
    "\n",
    "**DSRF + MCMIPF fusion with tabular, with MLflow tracking**\n",
    "\n",
    "**Optuna (JournalStorage + lock)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4f01581",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-16 13:32:03.385903: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-11-16 13:32:03.396340: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1763317923.403986 1939700 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1763317923.406482 1939700 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-11-16 13:32:03.416619: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import os, json, math, time, random, warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, callbacks\n",
    "\n",
    "import mlflow\n",
    "import mlflow.keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a407134",
   "metadata": {},
   "source": [
    "### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9da59b56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF GPUs: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "MLflow tracking: /mnt/SOLARLAB/E_Ladino/Repo_2/solar-forecasting-colombia/outputs/mlruns\n"
     ]
    }
   ],
   "source": [
    "SEED = 42\n",
    "random.seed(SEED); np.random.seed(SEED); tf.random.set_seed(SEED)\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "for g in gpus:\n",
    "    try:\n",
    "        tf.config.experimental.set_memory_growth(g, True)\n",
    "    except Exception:\n",
    "        pass\n",
    "print(\"TF GPUs:\", gpus)\n",
    "\n",
    "# Rutas\n",
    "ROOT = Path(\"..\").resolve()\n",
    "DATA_CLEAN = ROOT / \"data\" / \"clean\" / \"base_dataset.csv\"\n",
    "GOES_DIR = ROOT / \"data\" / \"GOES_v2\"\n",
    "\n",
    "OUT_DIR = ROOT / \"outputs\"\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "ART_DIR = OUT_DIR / \"artifacts_satellite\"\n",
    "ART_DIR.mkdir(parents=True, exist_ok=True)\n",
    "FIG_DIR = OUT_DIR / \"figures\"\n",
    "FIG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Target y ventanas\n",
    "TARGET_COL = \"GHI\"\n",
    "FREQ = \"10T\"\n",
    "DEFAULT_INPUT_STEPS   = 36   # 6h pasado (36 * 10min)\n",
    "DEFAULT_HORIZON_STEPS = 6    # 1h adelante\n",
    "\n",
    "PATIENCE = 6\n",
    "\n",
    "# MLflow\n",
    "MLFLOW_DIR = (OUT_DIR / \"mlruns\").resolve()\n",
    "mlflow.set_tracking_uri(\"file://\" + str(MLFLOW_DIR))\n",
    "mlflow.set_experiment(\"pg_industrial_satellite_fusion\")\n",
    "print(\"MLflow tracking:\", MLFLOW_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5089ec8b",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89551bd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total features used: 28\n",
      "['Presion', 'TempAmb', 'WindSpeed', 'WindDirection', 'hour_sin', 'hour_cos', 'DoY Sin', 'DoY Cos', 'solar_zenith', 'solar_azimuth', 'solar_elevation', 'TempAmb_roll1h_mean', 'TempAmb_roll6h_mean', 'Presion_roll1h_mean', 'Presion_roll6h_mean', 'WindSpeed_roll1h_mean', 'WindSpeed_roll6h_mean', 'temp_pressure_ratio', 'wind_temp_interaction', 'GHI_lag1', 'GHI_lag3', 'GHI_lag6', 'GHI_lag12', 'GHI_lag36', 'GHI_roll1h_mean', 'GHI_roll3h_mean', 'GHI_roll6h_mean', 'GHI_roll1h_max']\n",
      "N train/val/test: 75020 16076 16076\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(DATA_CLEAN, parse_dates=[0], index_col=0).sort_index()\n",
    "df.index.name = \"time\"\n",
    "\n",
    "# Features ya definidas en tu script anterior\n",
    "base_feats = [\n",
    "    'Presion','TempAmb','WindSpeed','WindDirection',\n",
    "    'hour_sin','hour_cos','DoY Sin','DoY Cos',\n",
    "    'solar_zenith','solar_azimuth','solar_elevation',\n",
    "    'TempAmb_roll1h_mean','TempAmb_roll6h_mean',\n",
    "    'Presion_roll1h_mean','Presion_roll6h_mean',\n",
    "    'WindSpeed_roll1h_mean','WindSpeed_roll6h_mean',\n",
    "    'temp_pressure_ratio','wind_temp_interaction'\n",
    "]\n",
    "ghi_lags  = [c for c in ['GHI_lag1','GHI_lag3','GHI_lag6','GHI_lag12','GHI_lag36'] if c in df.columns]\n",
    "ghi_rolls = [c for c in ['GHI_roll1h_mean','GHI_roll3h_mean','GHI_roll6h_mean','GHI_roll1h_max'] if c in df.columns]\n",
    "feat_cols = [c for c in base_feats if c in df.columns] + ghi_lags + ghi_rolls\n",
    "print(f\"Total features used: {len(feat_cols)}\")\n",
    "print(feat_cols)\n",
    "\n",
    "assert TARGET_COL in df.columns, f\"TARGET_COL='{TARGET_COL}' no existe en el dataset\"\n",
    "\n",
    "# Split temporal\n",
    "n = len(df); i_tr = int(0.7*n); i_va = int(0.85*n)\n",
    "df_train, df_val, df_test = df.iloc[:i_tr], df.iloc[i_tr:i_va], df.iloc[i_va:]\n",
    "\n",
    "# Escalado\n",
    "X_scaler = StandardScaler(); y_scaler = StandardScaler()\n",
    "X_train = X_scaler.fit_transform(df_train[feat_cols].values)\n",
    "X_val   = X_scaler.transform(df_val[feat_cols].values)\n",
    "X_test  = X_scaler.transform(df_test[feat_cols].values)\n",
    "\n",
    "y_train = y_scaler.fit_transform(df_train[[TARGET_COL]].values).ravel()\n",
    "y_val   = y_scaler.transform(df_val[[TARGET_COL]].values).ravel()\n",
    "y_test  = y_scaler.transform(df_test[[TARGET_COL]].values).ravel()\n",
    "\n",
    "# Imputación robusta\n",
    "imp = SimpleImputer(strategy=\"median\")\n",
    "X_train = imp.fit_transform(X_train)\n",
    "X_val   = imp.transform(X_val)\n",
    "X_test  = imp.transform(X_test)\n",
    "\n",
    "for name, arr in [(\"X_train\",X_train),(\"X_val\",X_val),(\"X_test\",X_test),\n",
    "                  (\"y_train\",y_train),(\"y_val\",y_val),(\"y_test\",y_test)]:\n",
    "    assert np.isfinite(arr).all(), f\"{name} tiene NaN/Inf\"\n",
    "\n",
    "# Guardamos índices para alinear con imágenes\n",
    "time_index = df.index\n",
    "time_train, time_val, time_test = time_index[:i_tr], time_index[i_tr:i_va], time_index[i_va:]\n",
    "print(\"N train/val/test:\", len(time_train), len(time_val), len(time_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f344dda",
   "metadata": {},
   "source": [
    "### Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c0c49aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics_from_scaled(pred_scaled, true_scaled, y_scaler):\n",
    "    p = y_scaler.inverse_transform(pred_scaled.reshape(-1,1)).ravel()\n",
    "    t = y_scaler.inverse_transform(true_scaled.reshape(-1,1)).ravel()\n",
    "    mae = mean_absolute_error(t, p)\n",
    "    rmse = float(np.sqrt(mean_squared_error(t, p)))\n",
    "    mape = float(np.mean(np.abs((t + 1e-6) - p) / (np.abs(t) + 1e-6)) * 100)\n",
    "    smape = float(100 * np.mean(2*np.abs(p - t) / (np.abs(t) + np.abs(p) + 1e-6)))\n",
    "    r2 = float(r2_score(t, p))\n",
    "    return {\"MAE\": mae, \"RMSE\": rmse, \"MAPE\": mape, \"sMAPE\": smape, \"R2\": r2}, (t, p)\n",
    "\n",
    "def _rmse(a,b): \n",
    "    return float(np.sqrt(mean_squared_error(a,b)))\n",
    "\n",
    "\n",
    "# ### Sequences (tabular + imágenes)\n",
    "\n",
    "# %%\n",
    "def build_seq_arrays_tabular(X_2d, y_1d, L, horizon):\n",
    "    \"\"\"\n",
    "    Tabular solamente (como tu build_seq_arrays original, pero sin imágenes).\n",
    "    \"\"\"\n",
    "    N, F = X_2d.shape\n",
    "    outX, outy = [], []\n",
    "    last = N - L - horizon + 1\n",
    "    if last <= 0:\n",
    "        return np.zeros((0, L, F), dtype=\"float32\"), np.zeros((0,), dtype=\"float32\")\n",
    "    for i in range(last):\n",
    "        block = X_2d[i:i+L]\n",
    "        outX.append(block)\n",
    "        outy.append(y_1d[i + L + horizon - 1])\n",
    "    return np.asarray(outX, dtype=\"float32\"), np.asarray(outy, dtype=\"float32\")\n",
    "\n",
    "\n",
    "def build_seq_arrays_fusion(X_2d, y_1d, imgs_4d, L, horizon):\n",
    "    \"\"\"\n",
    "    Secuencias sincronizadas tabular + imágenes.\n",
    "\n",
    "    X_2d   : (N, F_tab)\n",
    "    y_1d   : (N,)\n",
    "    imgs_4d: (N, H, W, C)\n",
    "    L      : input_steps\n",
    "    horizon: horizon_steps\n",
    "\n",
    "    Devuelve:\n",
    "      X_tab_seq: (N', L, F_tab)\n",
    "      X_img_seq: (N', L, H, W, C)\n",
    "      y_seq    : (N',)\n",
    "\n",
    "    Nota: se SALTAN las ventanas donde imgs_4d tiene NaNs\n",
    "          (huecos de NOAA).\n",
    "    \"\"\"\n",
    "    N, F = X_2d.shape\n",
    "    assert imgs_4d.shape[0] == N, \"imgs_4d y X_2d no alinean en N\"\n",
    "    H, W, C = imgs_4d.shape[1:]\n",
    "    X_tab_seq, X_img_seq, y_seq = [], [], []\n",
    "    last = N - L - horizon + 1\n",
    "    if last <= 0:\n",
    "        return (np.zeros((0, L, F), dtype=\"float32\"),\n",
    "                np.zeros((0, L, H, W, C), dtype=\"float32\"),\n",
    "                np.zeros((0,), dtype=\"float32\"))\n",
    "    skipped = 0\n",
    "    for i in range(last):\n",
    "        block_tab = X_2d[i:i+L]\n",
    "        block_img = imgs_4d[i:i+L]\n",
    "        # si hay NaNs en el bloque de imágenes, saltamos esa ventana\n",
    "        if np.isnan(block_img).any():\n",
    "            skipped += 1\n",
    "            continue\n",
    "        X_tab_seq.append(block_tab)\n",
    "        X_img_seq.append(block_img)\n",
    "        y_seq.append(y_1d[i + L + horizon - 1])\n",
    "    if skipped > 0:\n",
    "        print(f\"build_seq_arrays_fusion: ventanas saltadas por NaNs en imágenes = {skipped}\")\n",
    "    return (np.asarray(X_tab_seq, dtype=\"float32\"),\n",
    "            np.asarray(X_img_seq, dtype=\"float32\"),\n",
    "            np.asarray(y_seq,     dtype=\"float32\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3033f37a",
   "metadata": {},
   "source": [
    "### Satellite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d3eacfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "DSRF_DIR   = GOES_DIR / \"DSRF\"\n",
    "MCMIPF_DIR = GOES_DIR / \"MCMIPF\"\n",
    "\n",
    "def build_dsrf_by_hour(time_index):\n",
    "    \"\"\"\n",
    "    Construye:\n",
    "      - dsrf_all: (N, 128,128,1)\n",
    "      - también imprime cuántas horas faltan.\n",
    "\n",
    "    Estrategia:\n",
    "      1. Tomamos las horas únicas de df.index (floor a hora).\n",
    "      2. Cargamos cada archivo DSRF solo UNA vez por hora.\n",
    "      3. Para horas sin archivo, ponemos imagen llena de NaNs.\n",
    "      4. Luego expandimos a todos los timestamps (10-min).\n",
    "    \"\"\"\n",
    "    # horas únicas\n",
    "    hours = time_index.floor(\"H\")\n",
    "    unique_hours = sorted(hours.unique())\n",
    "\n",
    "    # mapa hora -> imagen (128x128x1)\n",
    "    hour_to_img = {}\n",
    "    missing_hours = 0\n",
    "\n",
    "    for h in unique_hours:\n",
    "        key = h.strftime(\"%Y%m%d_%H\")\n",
    "        year = key[:4]\n",
    "        month = key[4:6]\n",
    "        fname = f\"{key}_DSRF.npz\"\n",
    "        path = DSRF_DIR / year / month / fname\n",
    "        if not path.exists():\n",
    "            # hora sin DSRF -> NaNs\n",
    "            hour_to_img[key] = np.full((128,128,1), np.nan, dtype=\"float32\")\n",
    "            missing_hours += 1\n",
    "            continue\n",
    "        data = np.load(path)\n",
    "        arr = data[list(data.files)[0]]   # (1,256,256)\n",
    "        img = arr[0].astype(\"float32\")[::2, ::2]  # 256 -> 128\n",
    "        hour_to_img[key] = img[..., None]        # (128,128,1)\n",
    "\n",
    "    print(f\"DSRF: horas únicas en df      = {len(unique_hours)}\")\n",
    "    print(f\"DSRF: horas sin archivo NOAA  = {missing_hours}\")\n",
    "\n",
    "    # expandir a todos los timestamps (uno por fila de df)\n",
    "    imgs = []\n",
    "    for ts in time_index:\n",
    "        key = ts.strftime(\"%Y%m%d_%H\")\n",
    "        imgs.append(hour_to_img[key])\n",
    "    dsrf_all = np.stack(imgs, axis=0).astype(\"float32\")\n",
    "    return dsrf_all\n",
    "\n",
    "def build_mcmipf_by_hour(time_index):\n",
    "    \"\"\"\n",
    "    Igual idea para MCMIPF, pero:\n",
    "      - cada hora -> tensor (6,128,128,16)\n",
    "      - para cada timestamp (10-min) elegimos el slot 0..5.\n",
    "    \"\"\"\n",
    "    hours = time_index.floor(\"H\")\n",
    "    unique_hours = sorted(hours.unique())\n",
    "\n",
    "    hour_to_seq = {}\n",
    "    missing_hours = 0\n",
    "\n",
    "    for h in unique_hours:\n",
    "        key = h.strftime(\"%Y%m%d_%H\")\n",
    "        year = key[:4]\n",
    "        month = key[4:6]\n",
    "        fname = f\"{key}_MCMIPF.npz\"\n",
    "        path = MCMIPF_DIR / year / month / fname\n",
    "        if not path.exists():\n",
    "            hour_to_seq[key] = None\n",
    "            missing_hours += 1\n",
    "            continue\n",
    "        data = np.load(path)\n",
    "        arr = data[list(data.files)[0]]  # (6,16,256,256)\n",
    "        arr = arr.astype(\"float32\")[:, :, ::2, ::2]  # 256->128\n",
    "        arr = np.transpose(arr, (0, 2, 3, 1))        # (6,128,128,16)\n",
    "        hour_to_seq[key] = arr\n",
    "\n",
    "    print(f\"MCMIPF: horas únicas en df      = {len(unique_hours)}\")\n",
    "    print(f\"MCMIPF: horas sin archivo NOAA  = {missing_hours}\")\n",
    "\n",
    "    imgs = []\n",
    "    for ts in time_index:\n",
    "        key = ts.strftime(\"%Y%m%d_%H\")\n",
    "        seq = hour_to_seq[key]\n",
    "        if seq is None:\n",
    "            # sin MCMIPF -> NaNs\n",
    "            imgs.append(np.full((128,128,16), np.nan, dtype=\"float32\"))\n",
    "            continue\n",
    "        slot = ts.minute // 10  # 0..5\n",
    "        imgs.append(seq[slot])\n",
    "    mcm_all = np.stack(imgs, axis=0).astype(\"float32\")\n",
    "    return mcm_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51596d43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ Cargando DSRF completo (por hora)...\n",
      "DSRF: horas únicas en df      = 17862\n",
      "DSRF: horas sin archivo NOAA  = 237\n",
      "DSRF_all: (107172, 128, 128, 1)\n",
      "→ Cargando MCMIPF completo (por hora)...\n"
     ]
    }
   ],
   "source": [
    "# ### Construcción de tensores alineados\n",
    "\n",
    "print(\"→ Cargando DSRF completo (por hora)...\")\n",
    "dsrf_all = build_dsrf_by_hour(time_index)\n",
    "print(\"DSRF_all:\", dsrf_all.shape)  # (N,128,128,1)\n",
    "\n",
    "print(\"→ Cargando MCMIPF completo (por hora)...\")\n",
    "mcmipf_all = build_mcmipf_by_hour(time_index)\n",
    "print(\"MCMIPF_all:\", mcmipf_all.shape)  # (N,128,128,16)\n",
    "\n",
    "# Split\n",
    "dsrf_train, dsrf_val, dsrf_test = dsrf_all[:i_tr], dsrf_all[i_tr:i_va], dsrf_all[i_va:]\n",
    "mcm_train, mcm_val, mcm_test   = mcmipf_all[:i_tr], mcmipf_all[i_tr:i_va], mcmipf_all[i_va:]\n",
    "\n",
    "for name, arr in [(\"dsrf_train\",dsrf_train),(\"dsrf_val\",dsrf_val),(\"dsrf_test\",dsrf_test),\n",
    "                  (\"mcm_train\",mcm_train),(\"mcm_val\",mcm_val),(\"mcm_test\",mcm_test)]:\n",
    "    assert np.isfinite(arr).all() or np.isnan(arr).any(), f\"{name} tiene valores no finitos raros\"\n",
    "print(\"Satellite tensors listos.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c844531",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9206f40",
   "metadata": {},
   "source": [
    "#### 1) ConvLSTM con DSRF (solo satélite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4720a572",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "def build_convlstm_dsrf(L, H=128, W=128, C=1, filters1=32, filters2=64, k=5):\n",
    "    \"\"\"\n",
    "    ConvLSTM encoder-forecast style simple.\n",
    "    Input: (L, H, W, C)\n",
    "    Output: (1,) GHI (escalar) via GlobalPooling + Dense.\n",
    "    \"\"\"\n",
    "    inp = layers.Input(shape=(L, H, W, C))\n",
    "\n",
    "    x = layers.ConvLSTM2D(filters=filters1, kernel_size=(k,k),\n",
    "                          padding=\"same\", return_sequences=True,\n",
    "                          activation=\"relu\")(inp)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "\n",
    "    x = layers.ConvLSTM2D(filters=filters2, kernel_size=(k,k),\n",
    "                          padding=\"same\", return_sequences=False,\n",
    "                          activation=\"relu\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "\n",
    "    # mapa final HxWxfilters2 -> pooling -> Dense\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    x = layers.Dense(64, activation=\"relu\")(x)\n",
    "    out = layers.Dense(1, dtype=\"float32\")(x)\n",
    "\n",
    "    model = models.Model(inp, out, name=\"ConvLSTM_DSRF\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f759e9ac",
   "metadata": {},
   "source": [
    "#### 2) VGG19 + LSTM (MCMIPF + Tabular fusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa59c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import VGG19\n",
    "\n",
    "def build_vgg19_lstm_fusion(L, n_feat_tab, H=128, W=128, C=16,\n",
    "                            lstm_units_img=64, lstm_units_tab=64,\n",
    "                            dense_units=64, train_base=False):\n",
    "    \"\"\"\n",
    "    Fusiona:\n",
    "      - rama imágenes: MCMIPF -> Conv2D 1x1 (16->3) -> VGG19 (no top) -> LSTM\n",
    "      - rama tabular: LSTM\n",
    "      - concat -> Dense -> 1\n",
    "    \"\"\"\n",
    "    # --- Rama imágenes ---\n",
    "    inp_img = layers.Input(shape=(L, H, W, C), name=\"img_seq\")\n",
    "\n",
    "    # 16 canales -> 3 canales\n",
    "    x_img = layers.TimeDistributed(\n",
    "        layers.Conv2D(3, (1,1), padding=\"same\", activation=\"linear\"),\n",
    "        name=\"td_1x1_conv\"\n",
    "    )(inp_img)\n",
    "\n",
    "    # VGG19 base\n",
    "    vgg_base = VGG19(include_top=False, weights=\"imagenet\",\n",
    "                     input_shape=(H, W, 3))\n",
    "    vgg_base.trainable = train_base\n",
    "\n",
    "    x_img = layers.TimeDistributed(vgg_base, name=\"td_vgg19\")(x_img)\n",
    "    x_img = layers.TimeDistributed(layers.GlobalAveragePooling2D(),\n",
    "                                   name=\"td_gap\")(x_img)\n",
    "    # Ahora shape: (batch, L, feat_vgg)\n",
    "    x_img = layers.LSTM(lstm_units_img, activation=\"tanh\",\n",
    "                        name=\"lstm_img\")(x_img)\n",
    "\n",
    "    # --- Rama tabular ---\n",
    "    inp_tab = layers.Input(shape=(L, n_feat_tab), name=\"tab_seq\")\n",
    "    x_tab = layers.LSTM(lstm_units_tab, activation=\"tanh\",\n",
    "                        name=\"lstm_tab\")(inp_tab)\n",
    "\n",
    "    # --- Fusion ---\n",
    "    x = layers.Concatenate(name=\"fusion\")([x_img, x_tab])\n",
    "    x = layers.Dense(dense_units, activation=\"relu\")(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    out = layers.Dense(1, dtype=\"float32\", name=\"y_hat\")(x)\n",
    "\n",
    "    model = models.Model([inp_img, inp_tab], out, name=\"VGG19_LSTM_Fusion\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b899d4be",
   "metadata": {},
   "source": [
    "### Sequences + Training + Mlflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb342a9",
   "metadata": {},
   "source": [
    "#### ConvLSTM con DSRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97fe7297",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "def train_convlstm_dsrf(input_steps=DEFAULT_INPUT_STEPS,\n",
    "                        horizon_steps=DEFAULT_HORIZON_STEPS,\n",
    "                        lr=1e-3,\n",
    "                        batch_size=16,\n",
    "                        epochs=40):\n",
    "\n",
    "    # Construir secuencias con DSRF (solo satélite, ignorando tabular)\n",
    "    Xtr_seq, ytr_seq = build_seq_arrays_tabular(\n",
    "        X_2d=np.zeros_like(X_train),  # dummy (no usamos tabular aquí)\n",
    "        y_1d=y_train,\n",
    "        L=input_steps,\n",
    "        horizon=horizon_steps\n",
    "    )\n",
    "    Xva_seq, yva_seq = build_seq_arrays_tabular(\n",
    "        X_2d=np.zeros_like(X_val),\n",
    "        y_1d=y_val,\n",
    "        L=input_steps,\n",
    "        horizon=horizon_steps\n",
    "    )\n",
    "    # Pero necesitamos las imágenes: construimos con mismo esquema\n",
    "    # Nuestro build_seq_arrays_fusion se encarga de alinear imágenes y y's\n",
    "    _, Xtr_img_seq, ytr_seq = build_seq_arrays_fusion(\n",
    "        X_2d=np.zeros_like(X_train), y_1d=y_train,\n",
    "        imgs_4d=dsrf_train, L=input_steps, horizon=horizon_steps\n",
    "    )\n",
    "    _, Xva_img_seq, yva_seq = build_seq_arrays_fusion(\n",
    "        X_2d=np.zeros_like(X_val), y_1d=y_val,\n",
    "        imgs_4d=dsrf_val, L=input_steps, horizon=horizon_steps\n",
    "    )\n",
    "\n",
    "    print(\"ConvLSTM DSRF shapes:\")\n",
    "    print(\"Xtr_img_seq:\", Xtr_img_seq.shape)\n",
    "    print(\"ytr_seq    :\", ytr_seq.shape)\n",
    "\n",
    "    H, W, C = Xtr_img_seq.shape[2:]\n",
    "    model = build_convlstm_dsrf(L=input_steps, H=H, W=W, C=C)\n",
    "\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(lr),\n",
    "                  loss=\"mse\")\n",
    "\n",
    "    ckpt = (ART_DIR / \"best_convlstm_dsrf.weights.h5\").resolve()\n",
    "    cbs = [\n",
    "        callbacks.EarlyStopping(monitor=\"val_loss\", patience=PATIENCE,\n",
    "                                restore_best_weights=True, verbose=1),\n",
    "        callbacks.ModelCheckpoint(filepath=str(ckpt), monitor=\"val_loss\",\n",
    "                                  save_best_only=True, save_weights_only=True)\n",
    "    ]\n",
    "\n",
    "    with mlflow.start_run(run_name=\"ConvLSTM_DSRF\"):\n",
    "        mlflow.log_param(\"input_steps\",   input_steps)\n",
    "        mlflow.log_param(\"horizon_steps\", horizon_steps)\n",
    "        mlflow.log_param(\"lr\",           lr)\n",
    "        mlflow.log_param(\"batch_size\",   batch_size)\n",
    "        mlflow.log_param(\"epochs\",       epochs)\n",
    "\n",
    "        hist = model.fit(\n",
    "            Xtr_img_seq, ytr_seq,\n",
    "            validation_data=(Xva_img_seq, yva_seq),\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            verbose=1,\n",
    "            callbacks=cbs\n",
    "        )\n",
    "\n",
    "        # Test\n",
    "        _, Xte_img_seq, yte_seq = build_seq_arrays_fusion(\n",
    "            X_2d=np.zeros_like(X_test), y_1d=y_test,\n",
    "            imgs_4d=dsrf_test,\n",
    "            L=input_steps, horizon=horizon_steps\n",
    "        )\n",
    "        yhat_test_scaled = model.predict(Xte_img_seq, verbose=0).squeeze()\n",
    "        # yte_seq está en espacio escalado\n",
    "        metrics, (t_o, p_o) = metrics_from_scaled(yhat_test_scaled, yte_seq, y_scaler)\n",
    "\n",
    "        for k,v in metrics.items():\n",
    "            mlflow.log_metric(f\"test_{k}\", v)\n",
    "\n",
    "        # Guardar modelo\n",
    "        mlflow.keras.log_model(model, \"model\")\n",
    "\n",
    "        print(\"ConvLSTM DSRF test metrics:\", metrics)\n",
    "\n",
    "    return model, metrics, (t_o, p_o)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941aa8b5",
   "metadata": {},
   "source": [
    "#### VGG19 + LSTM + Tabular (MCMIPF fusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e512a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "def train_vgg19_lstm_fusion(input_steps=DEFAULT_INPUT_STEPS,\n",
    "                            horizon_steps=DEFAULT_HORIZON_STEPS,\n",
    "                            lr=5e-4,\n",
    "                            batch_size=4,\n",
    "                            epochs=25,\n",
    "                            train_base=False):\n",
    "\n",
    "    # Construimos secuencias tabulares + MCMIPF\n",
    "    Xtr_tab_seq, Xtr_img_seq, ytr_seq = build_seq_arrays_fusion(\n",
    "        X_2d=X_train, y_1d=y_train,\n",
    "        imgs_4d=mcm_train, L=input_steps, horizon=horizon_steps\n",
    "    )\n",
    "    Xva_tab_seq, Xva_img_seq, yva_seq = build_seq_arrays_fusion(\n",
    "        X_2d=X_val, y_1d=y_val,\n",
    "        imgs_4d=mcm_val, L=input_steps, horizon=horizon_steps\n",
    "    )\n",
    "\n",
    "    print(\"Fusion shapes:\")\n",
    "    print(\"Xtr_tab_seq:\", Xtr_tab_seq.shape)\n",
    "    print(\"Xtr_img_seq:\", Xtr_img_seq.shape)\n",
    "    print(\"ytr_seq    :\", ytr_seq.shape)\n",
    "\n",
    "    L = input_steps\n",
    "    n_feat_tab = Xtr_tab_seq.shape[2]\n",
    "    H, W, C = Xtr_img_seq.shape[2:]\n",
    "\n",
    "    model = build_vgg19_lstm_fusion(\n",
    "        L=L, n_feat_tab=n_feat_tab,\n",
    "        H=H, W=W, C=C,\n",
    "        lstm_units_img=64,\n",
    "        lstm_units_tab=64,\n",
    "        dense_units=64,\n",
    "        train_base=train_base\n",
    "    )\n",
    "\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(lr),\n",
    "                  loss=\"mse\")\n",
    "\n",
    "    ckpt = (ART_DIR / \"best_vgg19_lstm_fusion.weights.h5\").resolve()\n",
    "    cbs = [\n",
    "        callbacks.EarlyStopping(monitor=\"val_loss\", patience=PATIENCE+2,\n",
    "                                restore_best_weights=True, verbose=1),\n",
    "        callbacks.ModelCheckpoint(filepath=str(ckpt), monitor=\"val_loss\",\n",
    "                                  save_best_only=True, save_weights_only=True)\n",
    "    ]\n",
    "\n",
    "    with mlflow.start_run(run_name=\"VGG19_LSTM_Fusion\"):\n",
    "        mlflow.log_param(\"input_steps\",   input_steps)\n",
    "        mlflow.log_param(\"horizon_steps\", horizon_steps)\n",
    "        mlflow.log_param(\"lr\",           lr)\n",
    "        mlflow.log_param(\"batch_size\",   batch_size)\n",
    "        mlflow.log_param(\"epochs\",       epochs)\n",
    "        mlflow.log_param(\"train_base\",   train_base)\n",
    "        mlflow.log_param(\"img_H\",        H)\n",
    "        mlflow.log_param(\"img_W\",        W)\n",
    "        mlflow.log_param(\"img_C\",        C)\n",
    "\n",
    "        hist = model.fit(\n",
    "            {\"img_seq\": Xtr_img_seq, \"tab_seq\": Xtr_tab_seq},\n",
    "            ytr_seq,\n",
    "            validation_data=(\n",
    "                {\"img_seq\": Xva_img_seq, \"tab_seq\": Xva_tab_seq},\n",
    "                yva_seq\n",
    "            ),\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            verbose=1,\n",
    "            callbacks=cbs\n",
    "        )\n",
    "\n",
    "        # Test\n",
    "        Xte_tab_seq, Xte_img_seq, yte_seq = build_seq_arrays_fusion(\n",
    "            X_2d=X_test, y_1d=y_test,\n",
    "            imgs_4d=mcm_test,\n",
    "            L=input_steps, horizon=horizon_steps\n",
    "        )\n",
    "        yhat_test_scaled = model.predict(\n",
    "            {\"img_seq\": Xte_img_seq, \"tab_seq\": Xte_tab_seq},\n",
    "            verbose=0\n",
    "        ).squeeze()\n",
    "\n",
    "        metrics, (t_o, p_o) = metrics_from_scaled(yhat_test_scaled, yte_seq, y_scaler)\n",
    "        for k,v in metrics.items():\n",
    "            mlflow.log_metric(f\"test_{k}\", v)\n",
    "\n",
    "        mlflow.keras.log_model(model, \"model\")\n",
    "\n",
    "        print(\"VGG19+LSTM fusion test metrics:\", metrics)\n",
    "\n",
    "    return model, metrics, (t_o, p_o)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e62723e5",
   "metadata": {},
   "source": [
    "### Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ede681",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_sample_series(y_true, y_pred, title, n=1000, fname=None):\n",
    "    n = min(n, len(y_true))\n",
    "    plt.figure(figsize=(11,3.8))\n",
    "    plt.plot(y_true[:n], label=\"Real\", lw=1.5)\n",
    "    plt.plot(y_pred[:n], label=\"Pred\", lw=1.2, alpha=0.9)\n",
    "    plt.title(title); plt.xlabel(\"Time steps (10-min)\")\n",
    "    plt.ylabel(\"GHI (W/m²)\")\n",
    "    plt.legend(frameon=False); plt.tight_layout()\n",
    "    if fname is not None:\n",
    "        plt.savefig(fname, dpi=140)\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "e_ladino",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
